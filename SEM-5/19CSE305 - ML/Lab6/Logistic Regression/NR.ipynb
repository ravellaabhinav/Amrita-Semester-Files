{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NR.ipynb","provenance":[],"authorship_tag":"ABX9TyMxFMXgV2lo0APGFu6K/D2Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"H057AjA-4eyP"},"source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","class LogisticRegression:\n","    def __init__(self, X):\n","        self.beta_old_i = []\n","        #initializing b_i, always one additional coefficient than number of features of predictor\n","        #because eq β_0 + β_1*x having two coefficients β_0, β_1 where x has only one dimension\n","        self.beta_new_i = np.zeros(X.shape[1] + 1)\n","    \n","    #p(x) = e^(β_0 + β_1*x)/(1 + e^(β_0 + β_1*x))\n","    def probabilityFun(self, X):\n","        z = np.dot(self.beta_new_i, X.T)\n","        p = math.e**z/(1 + math.e**z)\n","        return p\n","\n","    #f'(β_j) = dl/d(β_j) = (i=1 to N)_Σ (y_i - p(x_i))*x_ij\n","    def firstDerivative(self, X, Y, P):\n","        firstDer = np.dot((Y-P), X)\n","        return firstDer\n","\n","    #f''(β_k) = dl/d(β_j)d(β_k) = - (i=1 to N)_Σ x_ij*x_ik*p(x_i)*(1 - p(x_i))\n","    def secondDerivative(self, X, P):\n","        probMul = P*(1-P)\n","        xMulp = np.array([x*y for (x,y) in zip(X, probMul)])\n","        secondDer = -1*np.dot(xMulp.T,X)\n","        return secondDer\n","\n","    #β_(i+1) = β_i - (f'(β_i))/(f''(β_i))\n","    def newtonRaphson(self, firstDer, secondDer):\n","        self.beta_new_i = self.beta_old_i - np.dot(linalg.inv(secondDer), firstDer)\n","#All functions have been defined as per derived formulas. Now let’s write method which does iterative process until coefficients are stabilized.\n","\n","#training the model\n","def fit(self, X, Y, maxIteration=50, diffThreshHold=10**-5):\n","    #adding one additional column since we will have additional coefficient\n","    X = np.c_[X, np.array([1]*X.shape[0])]\n","    iteration = 0\n","    diffBetaList = []\n","\n","    while(list(self.beta_new_i) != list(self.beta_old_i)):\n","        self.beta_old_i = self.beta_new_i\n","        P = self.probabilityFun(X)\n","        firstDer = self.firstDerivative(X, Y, P)\n","        secondDer = self.secondDerivative(X, P)\n","        self.newtonRaphson(firstDer, secondDer)\n","        #difference between last calcuated coefficients and current coefficients\n","        diff = linalg.norm(self.beta_new_i - self.beta_old_i)\n","        diffBetaList.append(diff)\n","        iteration += 1\n","        if(diff <= diffThreshHold or iteration > maxIteration):\n","            break\n","    \n","    return diffBetaList\n","#Now create a predict and classify method to calculate probability and classification.\n","\n","#predict probability any new data points\n","def predict(self, X):\n","    X = np.c_[X, np.array([1]*X.shape[0])]\n","    probability = self.probabilityFun(X)\n","    return probability\n","\n","#classify based on provided classes\n","def classify(self, X, dataClass):\n","    Y = self.predict(X)\n","    #if probability is less than 0.5 than categorized as class one else class two\n","    return [0 if item <= 0.05 else 1 for item in Y]\n","#Finally training and testing above code using iris data, I will use only two classes of iris data:\n","\n","iris = datasets.load_iris()\n","#iris data is 50 each three classes so only taking to 100 for two classes\n","x_train, x_test, y_train, y_test = train_test_split(iris['data'][:100], iris['target'][:100])\n","reg = LogisticRegression(x_train)\n","reg.fit(x_train,y_train)\n","pred = reg.classify(x_test, iris[\"target_names\"][:2])\n","print(\"Accuracy: {:.2f}%\".format(100*np.mean(pred == y_test)))"],"execution_count":null,"outputs":[]}]}